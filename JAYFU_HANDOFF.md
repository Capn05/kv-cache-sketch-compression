# Project Handoff: KV-Cache Sketch Compression
**To**: Jay Fu  
**From**: Gabriel Alvarado  
**Date**: December 12, 2024  
**Project**: Probabilistic Sketching for KV-Cache Compression in LLMs

---

## Executive Summary

We implemented a Count-Min Sketch-based KV-cache compression system for GPT-2, including:
- ‚úÖ Working Count-Min Sketch data structure
- ‚úÖ Modified GPT-2 with sketch-based eviction policy
- ‚úÖ Comprehensive evaluation framework
- ‚úÖ 144 experimental configurations tested
- ‚ö†Ô∏è **Critical Issue**: Cache not populating during generation (integration problem)

**Bottom line**: The compression algorithm works in isolation, but doesn't integrate properly with HuggingFace's generation API. We have a proof-of-concept with real implementation challenges to discuss in the report.

---

## Project Status

### ‚úÖ What's Working

1. **Count-Min Sketch Implementation** (`src/sketches/count_min_sketch.py`)
   - GPU-accelerated operations
   - Universal hashing (prime = 2^31 - 1)
   - Top-k retrieval based on importance scores
   - Memory tracking

2. **Eviction Policy** (Tested separately)
   - When cache exceeds `max_cache_size`, evicts least important token
   - Uses sketch to query importance scores
   - Successfully limits storage (verified in unit tests)

3. **Evaluation Framework**
   - Memory tracking (GPU)
   - Latency measurement
   - Quality metrics (perplexity, BLEU)
   - Comprehensive logging

4. **Experiments Run**
   - 144 configurations tested
   - Multiple compression levels (max_cache_size: 64, 128, 256)
   - Baseline comparisons (sliding window, quantization)

### ‚ùå What's NOT Working

1. **Critical Issue: Empty Cache During Generation**
   ```
   Results show:
   Cache memory: 0.00 MB
   Avg tokens stored/layer: 0.0    ‚Üê Should be 64-256!
   ```
   
   **Problem**: The `SketchedCache.update()` method is never called during `model.generate()`. The HuggingFace generation API uses a different code path that doesn't trigger our cache population logic.

2. **No Actual Compression Visible**
   - All configurations show ~273-278 MB memory
   - Model weights (~240 MB) dominate measurements
   - Cache differences lost in noise with short sequences (64 tokens)

3. **Attention Analysis Fails**
   - Getting `'NoneType' object has no attribute 'shape'` errors
   - Not critical since main experiments completed

---

## Implementation Overview

### Architecture

```
GPT-2 Model (240 MB)
‚îú‚îÄ‚îÄ 12 Transformer Layers
‚îÇ   ‚îî‚îÄ‚îÄ SketchGPT2Attention
‚îÇ       ‚îî‚îÄ‚îÄ SketchedCache (per layer)
‚îÇ           ‚îú‚îÄ‚îÄ Count-Min Sketch (importance tracking)
‚îÇ           ‚îú‚îÄ‚îÄ stored_keys[] (LIMITED to max_cache_size)
‚îÇ           ‚îú‚îÄ‚îÄ stored_values[] (LIMITED to max_cache_size)
‚îÇ           ‚îî‚îÄ‚îÄ Eviction policy (removes least important)
```

### Key Design: Option A - Sketch-Based Eviction

**Concept**: Use sketch to track importance, maintain limited cache

```python
class SketchedCache:
    def __init__(self, max_cache_size=128):
        self.max_cache_size = 128  # Hard limit!
        self.sketch = CountMinSketch(...)
        self.stored_keys = []  # Only stores up to max_cache_size
        
    def update(self, key, value):
        # Add new token
        self.stored_keys.append(key)
        self.sketch.update(key, importance=1.0)
        
        # EVICT if over limit
        if len(self.stored_keys) > self.max_cache_size:
            self._evict_least_important()  # Uses sketch scores
```

**Expected compression**:
- max_cache_size=64 ‚Üí store only 64 tokens ‚Üí 75% reduction vs 256
- max_cache_size=128 ‚Üí store only 128 tokens ‚Üí 50% reduction

**Actual result**: Cache never populated (integration issue)

---

## File Structure

```
Final project/
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ sketches/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ count_min_sketch.py          # Sketch implementation (WORKING ‚úÖ)
‚îÇ   ‚îú‚îÄ‚îÄ models/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ sketch_gpt2.py               # Modified GPT-2 (ISSUE ‚ö†Ô∏è)
‚îÇ   ‚îî‚îÄ‚îÄ evaluation/
‚îÇ       ‚îî‚îÄ‚îÄ metrics.py                   # Metrics framework (WORKING ‚úÖ)
‚îÇ
‚îú‚îÄ‚îÄ experiments/
‚îÇ   ‚îú‚îÄ‚îÄ baseline.py                      # Full cache baseline (COMPLETE ‚úÖ)
‚îÇ   ‚îú‚îÄ‚îÄ sketch_experiments.py            # Main experiments (RAN ‚úÖ)
‚îÇ   ‚îú‚îÄ‚îÄ baselines_comparison.py          # Comparisons (COMPLETE ‚úÖ)
‚îÇ   ‚îî‚îÄ‚îÄ attention_analysis.py            # Distribution analysis (ERRORS ‚ö†Ô∏è)
‚îÇ
‚îú‚îÄ‚îÄ results/
‚îÇ   ‚îú‚îÄ‚îÄ baseline/                        # Baseline results
‚îÇ   ‚îú‚îÄ‚îÄ sketch/                          # 144 config results
‚îÇ   ‚îú‚îÄ‚îÄ baselines/                       # Comparison results
‚îÇ   ‚îî‚îÄ‚îÄ attention_analysis/              # Incomplete
‚îÇ
‚îú‚îÄ‚îÄ notebooks/
‚îÇ   ‚îî‚îÄ‚îÄ analysis.ipynb                   # Visualization (READY ‚úÖ)
‚îÇ
‚îú‚îÄ‚îÄ run_all_experiments.py               # Master runner (WORKING ‚úÖ)
‚îú‚îÄ‚îÄ PROJECT.md                           # Project report (NEEDS UPDATE üìù)
‚îú‚îÄ‚îÄ requirements.txt                     # Dependencies
‚îî‚îÄ‚îÄ README.md                            # Setup instructions
```

---

## Experimental Results

### Configuration
- **Model**: GPT-2 Small (117M parameters)
- **Device**: GPU (CUDA)
- **Mode**: Quick (3 samples per config)
- **Configs**: 144 total (width √ó depth √ó strategy √ó max_cache_size)

### Key Findings

| Method | Memory | Latency | Notes |
|--------|--------|---------|-------|
| **Full Cache (Baseline)** | ~275 MB | Not measured | Upper bound |
| **Sketch (max=64)** | 273 MB | 0.48s | Cache empty ‚ö†Ô∏è |
| **Sketch (max=128)** | 276 MB | 0.48s | Cache empty ‚ö†Ô∏è |
| **Sketch (max=256)** | 276 MB | 0.59s | Cache empty ‚ö†Ô∏è |
| **Sliding Window** | 278 MB | 3.89s | 8x slower! |
| **Quantization** | 275 MB | 3.43s | 7x slower! |

**Surprising finding**: Sketch configs show **much faster latency** (0.48s vs 3.43-3.89s), but this is likely a measurement artifact since cache is empty.

### What We Expected vs What We Got

**Expected**:
- max_cache_size=64: ~70 MB cache memory, 75% savings
- max_cache_size=128: ~140 MB cache memory, 50% savings
- Visible memory differences between configs

**Actual**:
- All configs: 273-278 MB (model weights dominate)
- Cache memory: 0.00 MB (empty!)
- Avg tokens/layer: 0.0 (should be 64-256)

---

## The Core Problem: Integration Issue

### What Should Happen

```python
# During generation, for each new token:
1. Compute attention ‚Üí get key, value vectors
2. Call sketch_cache.update(key, value)  ‚Üê Populate cache
3. sketch_cache checks if over max_cache_size
4. If yes, evict least important token
5. Store limited cache on GPU
```

### What Actually Happens

```python
# During generation:
1. HuggingFace's generate() uses standard code path
2. Our SketchedCache.update() is NEVER CALLED
3. Cache remains empty
4. No eviction occurs
5. Results don't show compression
```

### Why This Happened

The integration point is in `SketchGPT2Attention._attn()`:

```python
def _attn(self, query, key, value, attention_mask=None, head_mask=None):
    if self.use_sketch:
        # Update cache with new keys/values
        for h in range(num_heads):
            curr_key = key[:, h, -1, :]
            curr_value = value[:, h, -1, :]
            self.sketch_cache.update(curr_key, curr_value)  ‚Üê NOT TRIGGERED
```

**Issue**: The `_attn` method is called, but the logic that updates the cache isn't executing properly during `generate()`. The HuggingFace generation loop may be using cached past_key_values differently than we expected.

---

## Evolution of the Implementation

### Iteration 1: Initial Implementation
- Stored ALL keys/values + sketch for selection
- **Result**: No compression (stored everything)
- **Memory**: 273 MB (1.7% "savings" = noise)

### Iteration 2: Added Eviction (Option A)
- Added `max_cache_size` limit
- Implemented `_evict_least_important()`
- **Result**: Logic works in unit tests, but not during generation
- **Memory**: Still 273-278 MB

### Iteration 3: Moved Cache to GPU
- Changed from `.cpu()` to keep on GPU
- Added detailed memory breakdown
- **Result**: Can now track cache memory separately
- **Discovery**: Cache is empty (0.0 tokens/layer)

### Current Status: Iteration 3
- Eviction logic: ‚úÖ Works
- GPU tracking: ‚úÖ Works  
- Integration: ‚ùå Broken
- Compression: ‚ùå Not happening

---

## Technical Details

### Count-Min Sketch Parameters Tested

| Parameter | Values | Purpose |
|-----------|--------|---------|
| `sketch_width` | [128, 256, 512, 1024] | Hash table width |
| `sketch_depth` | [2, 4, 6] | Number of hash functions |
| `strategy` | ['topk', 'aggregate'] | Attention approximation |
| `topk` | [32, 64, 128] | Tokens to attend to |
| `max_cache_size` | [64, 128, 256] | **Compression knob** |

**Total**: 4 √ó 3 √ó 2 √ó 3 √ó 3 = 144 configurations (only 48 for aggregate)

### Memory Breakdown

**Model Components**:
- GPT-2 weights: ~240 MB (fixed)
- Activations: ~30-35 MB (varies)
- KV cache (full, 256 tokens): ~20 MB for 12 layers
- **Total**: ~273-278 MB

**With compression (theoretical)**:
- max_cache_size=64: ~5 MB cache (vs 20 MB)
- **Total expected**: ~258 MB (-7% vs baseline)

**Why not visible**: 
1. Model weights dominate (240/273 = 88%)
2. Short sequences in quick mode (64 tokens generated)
3. Cache empty due to integration issue

---

## What Still Needs To Be Done

### Critical Fixes

1. **Fix Cache Population** (HIGHEST PRIORITY)
   - **Option A**: Debug why `update()` isn't called during generation
   - **Option B**: Modify integration point in `_attn()` method
   - **Option C**: Bypass `generate()` and do manual token-by-token generation
   
   **Estimated time**: 4-8 hours
   **Difficulty**: High (requires deep dive into HuggingFace internals)

2. **Verify Compression with Working Cache**
   - Once cache populates, should see tokens/layer = max_cache_size
   - Memory should scale with max_cache_size
   - Need to re-run experiments

3. **Test with Longer Sequences**
   - Current: 64 tokens (cache tiny even uncompressed)
   - Need: 512-1024 tokens to see meaningful compression
   - Modify `run_all_experiments.py`: change `max_new_tokens=128` ‚Üí `max_new_tokens=512`

### Nice-to-Have

4. **Fix Attention Analysis**
   - Currently getting NoneType errors
   - Not critical but would be good for completeness

5. **Perplexity Measurements**
   - Compare quality with/without compression
   - Validate hypothesis about <2% degradation

6. **Update PROJECT.md**
   - Add actual results section
   - Discuss integration challenges discovered
   - Be honest about what worked vs didn't

---

## How to Run Everything

### Setup
```bash
# Install dependencies
py -m pip install -r requirements.txt

# Verify installation
py test_implementation.py  # Basic tests (should pass)
```

### Run Experiments
```bash
# Quick test (3 samples, ~20 min)
py run_all_experiments.py --quick --device cuda

# Full experiments (10+ samples, ~2 hours)
py run_all_experiments.py --device cuda
```

### Check Results
```bash
# Results are in results/ directory
results/
‚îú‚îÄ‚îÄ baseline/baseline_summary.json
‚îú‚îÄ‚îÄ sketch/experiment_summary.json
‚îî‚îÄ‚îÄ baselines/comparison_summary.json

# Visualize
jupyter notebook notebooks/analysis.ipynb
```

---

## Debugging the Integration Issue

### To investigate why cache is empty:

1. **Add debug prints** in `src/models/sketch_gpt2.py`:
```python
def update(self, key, value, attention_weight=None):
    print(f"UPDATE CALLED! Stored: {len(self.stored_keys)}")  # Add this
    self.stored_keys.append(key.detach())
    # ...
```

2. **Run simple test**:
```python
from src.models.sketch_gpt2 import SketchGPT2LMHeadModel
from transformers import GPT2Tokenizer

model = SketchGPT2LMHeadModel.from_pretrained('gpt2').to('cuda')
model.enable_sketch_cache(max_cache_size=32)

tokenizer = GPT2Tokenizer.from_pretrained('gpt2')
input_ids = tokenizer.encode("Hello world", return_tensors='pt').to('cuda')

outputs = model.generate(input_ids, max_new_tokens=50)
# Should see "UPDATE CALLED!" prints if working
```

3. **Check if `_attn` is called**:
   - Add print in `SketchGPT2Attention._attn()`
   - Verify it's executing during generation

4. **Inspect past_key_values**:
   - HuggingFace caches might bypass our custom cache
   - May need to modify how we integrate with `use_cache=True`

### Alternative Approach: Manual Generation Loop

If `generate()` is too complex, implement manual loop:

```python
def manual_generate(model, input_ids, max_tokens=100):
    for _ in range(max_tokens):
        outputs = model(input_ids, use_cache=False)  # No caching
        next_token = outputs.logits[:, -1, :].argmax(dim=-1, keepdim=True)
        input_ids = torch.cat([input_ids, next_token], dim=1)
        
        # Manually trigger cache update
        # ... extract keys/values and call sketch_cache.update()
    
    return input_ids
```

---

## Key Insights from Development

### What We Learned

1. **Sketch algorithms work** - Count-Min Sketch correctly tracks importance and retrieves top-k
2. **Eviction logic works** - When triggered, successfully limits cache size
3. **Integration is hard** - Modifying core transformer components in production frameworks is complex
4. **Measurement challenges** - Short sequences + model weight dominance mask compression effects
5. **Quick mode limitations** - Need longer sequences to see compression benefits

### Why This Is Still Valuable

Even though compression isn't visible in results:
- ‚úÖ **Comprehensive implementation** of sketch-based approach
- ‚úÖ **Working eviction policy** (verified in isolation)
- ‚úÖ **Extensive evaluation framework** (144 configs tested)
- ‚úÖ **Real implementation challenges** discovered (valuable for research)
- ‚úÖ **Modular codebase** ready for future extensions

### The Honest Story for the Report

This is actually a **strong research outcome**:

> "We implemented sketch-based KV-cache compression using Count-Min Sketch for importance tracking and an eviction policy maintaining a cache size limit. While the eviction mechanism works correctly in isolation, integration with HuggingFace's generation API proved challenging‚Äîthe cache update logic is not triggered during standard generation. This highlights a key finding: theoretical compression algorithms face significant practical hurdles when integrating with production ML frameworks. The gap between 'algorithm works' and 'system works end-to-end' represents an important research contribution, demonstrating why KV-cache optimization remains an open problem despite elegant theoretical solutions."

---

## Recommendations for Final Report

### What to Emphasize

1. **Implementation Completeness**
   - Full sketch-based system implemented
   - 144 experimental configurations
   - Comprehensive evaluation framework

2. **Design Decisions**
   - Why Count-Min Sketch (frequency estimation, sublinear space)
   - Option A (eviction) vs other approaches
   - Trade-offs explored

3. **Lessons Learned**
   - Integration challenges with existing frameworks
   - Measurement difficulties (short sequences, model weights)
   - Gap between theory and practice

### What to Acknowledge

1. **Limitations**
   - Cache population issue (be transparent)
   - No actual compression visible in experiments
   - Short sequences mask potential benefits

2. **Future Work**
   - Fix integration with generation API
   - Test with longer sequences (512-1024 tokens)
   - Implement Count-Sketch and RACE variants
   - Add adaptive controller

### Don't Hide the Problems!

In research, **negative results are valuable**. You discovered:
- Why KV-cache optimization is hard in practice
- Integration challenges with production frameworks
- Measurement subtleties in ML systems

These are legitimate contributions!

---

## Quick Reference

### Important Files
- `src/models/sketch_gpt2.py` - Core implementation (lines 18-230: SketchedCache)
- `experiments/sketch_experiments.py` - Main experiments
- `results/sketch/experiment_summary.json` - All results
- `PROJECT.md` - Report (needs update)

### Key Variables
- `max_cache_size` - Controls compression (64, 128, 256)
- `sketch_width` - Hash table width (128-1024)
- `sketch_depth` - Hash functions (2-6)
- `stored_keys` - Should contain max_cache_size tokens (currently empty!)

### Commands
```bash
# Test basic functionality
py test_implementation.py

# Run experiments
py run_all_experiments.py --quick --device cuda

# Analyze results
jupyter notebook notebooks/analysis.ipynb
```

---

## Questions for Discussion

1. **Should we fix the integration?** (4-8 hours work)
   - Or acknowledge limitation and focus on writing?

2. **Report strategy?**
   - Emphasize implementation + lessons learned?
   - Or try to get working compression first?

3. **Experiments to re-run?**
   - If we fix integration, rerun everything
   - Need longer sequences (512+ tokens)

4. **Scope for final report?**
   - Just Count-Min Sketch (what we have)
   - Or attempt Count-Sketch/RACE too?

---

## Contact & Handoff

**Implementation done by**: Gabriel Alvarado  
**Date**: December 11-12, 2024  
**Hours spent**: ~12 hours (implementation + experiments)

**Status**: 
- Core implementation: ‚úÖ Complete
- Experiments: ‚úÖ Run (144 configs)
- Integration: ‚ö†Ô∏è **Issue to resolve**
- Report: üìù Needs final writeup

**Next person's task**: 
1. Decide: fix integration or write about what we learned?
2. Update PROJECT.md with actual results
3. Create visualizations from results/
4. Write conclusions section

**Estimated time to completion**: 6-10 hours (depending on whether we fix integration)

---

## Summary

We built a complete sketch-based KV-cache compression system with 2,450+ lines of code, comprehensive experiments, and real findings about the challenges of ML systems optimization. The eviction algorithm works, but integration with HuggingFace revealed practical challenges. This is valuable research output‚Äîwe now know *why* this is hard, not just how to implement it.

**The code is there. The experiments ran. The results tell a story. Now we need to write it up honestly and completely.**

Good luck! üöÄ

---

**Document Version**: 1.0  
**Last Updated**: December 12, 2024  
**Status**: Ready for handoff

